#!/usr/bin/env python3
"""
Test simple pour vÃ©rifier que Groq fonctionne avec le modÃ¨le llama-3.1-8b-instant
"""

import os
import sys
from pathlib import Path

# Ajouter le rÃ©pertoire parent au path
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
sys.path.insert(0, str(parent_dir))

# Charger le fichier .env
env_file = parent_dir / '.env'
if env_file.exists():
    print(f"ğŸ“„ Chargement du fichier .env depuis: {env_file}")
    with open(env_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                os.environ[key.strip()] = value.strip()

def test_groq_working_model():
    """Tester uniquement le modÃ¨le qui fonctionne"""
    try:
        from openai import OpenAI
        
        groq_key = os.getenv('GROQ_API_KEY')
        if not groq_key:
            print("âŒ GROQ_API_KEY non dÃ©finie")
            return False
        
        print("ğŸ§ª Test Groq avec llama-3.1-8b-instant SEULEMENT")
        print("=" * 60)
        print(f"ğŸ”‘ ClÃ© Groq: {groq_key[:20]}...")
        
        # CrÃ©er le client OpenAI avec l'URL Groq
        client = OpenAI(
            api_key=groq_key,
            base_url="https://api.groq.com/openai/v1"
        )
        
        # Test 1: RequÃªte simple
        print("ğŸš€ Test 1: RequÃªte simple...")
        response = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "user", "content": "Dis juste 'Test rÃ©ussi!' en franÃ§ais."}
            ],
            max_tokens=20,
            temperature=0.1
        )
        
        result = response.choices[0].message.content
        print(f"âœ… RÃ©ponse: {result}")
        
        # Test 2: Analyse financiÃ¨re simple
        print("ğŸš€ Test 2: Analyse financiÃ¨re...")
        response2 = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "user", "content": "Analyse l'ETF SPY en 1 phrase courte."}
            ],
            max_tokens=50,
            temperature=0.3
        )
        
        result2 = response2.choices[0].message.content
        print(f"âœ… Analyse SPY: {result2}")
        
        # Test 3: Vitesse
        import time
        start_time = time.time()
        
        response3 = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "user", "content": "Recommandation: BUY, SELL ou HOLD pour SPY?"}
            ],
            max_tokens=30,
            temperature=0.2
        )
        
        end_time = time.time()
        result3 = response3.choices[0].message.content
        duration = end_time - start_time
        
        print(f"âœ… Recommandation: {result3}")
        print(f"âš¡ Temps de rÃ©ponse: {duration:.2f} secondes")
        
        print("ğŸ‰ Groq llama-3.1-8b-instant fonctionne parfaitement!")
        return True
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        return False

def main():
    """Fonction principale"""
    print("ğŸ§ª Test Simple Groq - ModÃ¨le Fonctionnel")
    print("=" * 60)
    
    success = test_groq_working_model()
    
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ SUCCÃˆS! Groq est prÃªt pour TradingAgents!")
        print("âœ… ModÃ¨le: llama-3.1-8b-instant")
        print("âœ… Vitesse: Ultra-rapide")
        print("âœ… Configuration: OptimisÃ©e")
        print("\nğŸš€ Vous pouvez maintenant:")
        print("1. DÃ©marrer l'application: python run.py")
        print("2. Tester une analyse avec SPY")
        print("3. Utiliser SEULEMENT l'Analyste MarchÃ© pour aller vite")
        print("4. L'analyse devrait prendre 15-30 secondes!")
    else:
        print("âŒ ProblÃ¨me avec Groq")
    print("=" * 60)

if __name__ == "__main__":
    main()
